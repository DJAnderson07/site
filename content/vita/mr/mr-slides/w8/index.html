<!DOCTYPE html>
<html>
  <head>
    <title>Multi-model inference</title>
    <meta charset="utf-8">
    <meta name="author" content="Daniel Anderson" />
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/uo.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/uo-fonts.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/hygge.css" rel="stylesheet" />
    <link rel="stylesheet" href="custom.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Multi-model inference
### Daniel Anderson
### Week 8

---




# Agenda 
* Review Exercise
* Introducing multi-model comparisons
  + Revisiting squared semi-partial correlations from a multi-model approach
  + Testing differences between models
* A brief foray into information criteria
* More model comparisons
  + Comparing blocks of predictors
  + Thinking more about interactions
* Lab

---
class: inverse middle center
background-image:url(../img/chalkboard.jpg)
background-size:cover

# What questions do you have?


---
class: inverse middle
background-image:url(../img/wood.jpg)
background-size:cover

# Let's get setup

### Tonight, we'll work with multiple data sources


---
# Revisiting `\(ssr\)`

--
### First, let's fit a model


```r
benchmarks &lt;- import(here("data", "benchmarks.xlsx"),
              setclass = "tbl_df") 

m1 &lt;- lm(rdg_spr ~ rdg_fall, benchmarks)
arm::display(m1, detail = TRUE)
```

```
## lm(formula = rdg_spr ~ rdg_fall, data = benchmarks)
##             coef.est coef.se t value Pr(&gt;|t|)
## (Intercept) 115.48    13.47    8.57    0.00  
## rdg_fall      0.44     0.07    6.18    0.00  
## ---
## n = 174, k = 2
## residual sd = 12.90, R-Squared = 0.18
```

--
### Pop Quiz
What would the squared semipartial correlation for `rdg_fall` be here?

---
# It's `\(R^2\)`!
(which is actually `\(r^2\)` in this case)

--

```r
library(lmSupport)
modelEffectSizes(m1)
```

```
## lm(formula = rdg_spr ~ rdg_fall, data = benchmarks)
## 
## Coefficients
##                   SSR df pEta-sqr dR-sqr
## (Intercept) 12229.492  1   0.2995     NA
## rdg_fall     6350.213  1   0.1817 0.1817
## 
## Sum of squared errors (SSE): 28607.2
## Sum of squared total  (SST): 34957.4
```

---
# Multi-model comparison
* Let's think about the previous example from a multi-model comparison
perspective.

--
* We want to compare how much **additional** variance is accounted for by the
model with the inclusion of `rdg_fall`. 

--
* What does that mean?

--
* Compare the difference between a model with no predictors

--
* What does it mean for a model to have no independent variables? What would we
be estimating?

--
.Large[
$$
rdg\_{spr} = b\_0 + e
$$
]

---


```r
m0 &lt;- lm(rdg_spr ~ 1, benchmarks)
arm::display(m0, detail = TRUE, digits = 4)
```

```
## lm(formula = rdg_spr ~ 1, data = benchmarks)
##             coef.est coef.se  t value  Pr(&gt;|t|)
## (Intercept) 198.4828   1.0776 184.1834   0.0000
## ---
## n = 174, k = 1
## residual sd = 14.2150, R-Squared = 0.00
```

--
What does the intercept represent here?


--
The grand mean for the outcome.


```r
mean(benchmarks$rdg_spr)
```

```
## [1] 198.4828
```

---
# Pop Quiz
What would the fitted values (predictions from the model for our observed 
cases) be for this model be?

--

```r
benchmarks %&gt;%
  mutate(m0_fitted = fitted(m0))
```

```
## # A tibble: 174 x 8
##       sid sped     ethnicity  frl     ell     rdg_fall rdg_spr m0_fitted
##     &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;
##  1 409687 Non-Sped Hispanic   FRL     Active       181     194  198.4828
##  2 409847 Sped     Black      FRL     Non-ELL      166     159  198.4828
##  3 491527 Sped     White      FRL     Non-ELL      216     198  198.4828
##  4 500147 Non-Sped Native Am. FRL     Non-ELL      203     204  198.4828
##  5 501027 Non-Sped Hispanic   FRL     Active       198     173  198.4828
##  6 501147 Non-Sped Hispanic   FRL     Active       188     173  198.4828
##  7 501217 Non-Sped Hispanic   FRL     Monitor      202     200  198.4828
##  8 501787 Non-Sped White      Non-FRL Active       182     206  198.4828
##  9 502417 Non-Sped Black      Non-FRL Non-ELL      194     191  198.4828
## 10 503127 Non-Sped Hispanic   FRL     Active       170     185  198.4828
## # ... with 164 more rows
```

---
# Why do we care?
* We can use it as a basis of comparison.

Let's look back at our output from `m1`

---

```r
summary(m1)
```

```
## 
## Call:
## lm(formula = rdg_spr ~ rdg_fall, data = benchmarks)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -34.883  -7.991   0.301   7.506  28.319 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 115.48480   13.46773   8.575 5.58e-15 ***
## rdg_fall      0.44044    0.07128   6.179 4.53e-09 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 12.9 on 172 degrees of freedom
## Multiple R-squared:  0.1817,	Adjusted R-squared:  0.1769 
*## F-statistic: 38.18 on 1 and 172 DF,  p-value: 4.527e-09
```

---
# Comparing models
We can use the `anova` function to compare models


```r
anova(m0, m1)
```

```
## Analysis of Variance Table
## 
## Model 1: rdg_spr ~ 1
## Model 2: rdg_spr ~ rdg_fall
##   Res.Df   RSS Df Sum of Sq     F    Pr(&gt;F)    
## 1    173 34957                                 
*## 2    172 28607  1    6350.2 38.18 4.527e-09 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```

--
Notice that the `\(F\)` test is exactly the same!

---
# Okay so... who cares?

.center[
![](https://i.gifer.com/3JFT.gif)
]

--
* We can get a direct test of significance for squared semi-partial correlations

--
* We can test if **blocks** of variables significantly reduce the residual
variance 

--
* We can directly compare parallel slopes models with non-parallel slopes 
  models (i.e., interactions)

---
## Let's think about a slightly more complicated model


```r
m2 &lt;- lm(rdg_spr ~ rdg_fall + sped, benchmarks)
arm::display(m2)
```

```
## lm(formula = rdg_spr ~ rdg_fall + sped, data = benchmarks)
##             coef.est coef.se
## (Intercept) 123.14    13.25 
## rdg_fall      0.40     0.07 
## spedSped    -18.06     5.26 
## ---
## n = 174, k = 3
## residual sd = 12.51, R-Squared = 0.23
```

---
# Look at the difference in `\(R^2\)`


```r
summary(m1)$r.squared
```

```
## [1] 0.1816555
```

```r
summary(m2)$r.squared
```

```
## [1] 0.2344025
```

--
### What's the unique variance accounted for by `sped`?


```r
summary(m2)$r.squared - summary(m1)$r.squared
```

```
## [1] 0.05274701
```

--
Is this value different than zero?

---
# Compare models


```r
anova(m1, m2)
```

```
## Analysis of Variance Table
## 
## Model 1: rdg_spr ~ rdg_fall
## Model 2: rdg_spr ~ rdg_fall + sped
##   Res.Df   RSS Df Sum of Sq      F    Pr(&gt;F)    
## 1    172 28607                                  
## 2    171 26763  1    1843.9 11.781 0.0007504 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```


--
### Challenge
* Can you calculate (any method you want) and test the significance of
`\(\Delta R^2\)` for `rdg_fall`?

---


```r
sped_mod &lt;- lm(rdg_spr ~ sped, benchmarks)
summary(m2)$r.squared - summary(sped_mod)$r.squared
```

```
## [1] 0.148498
```

```r
modelEffectSizes(m2)
```

```
## lm(formula = rdg_spr ~ rdg_fall + sped, data = benchmarks)
## 
## Coefficients
##                   SSR df pEta-sqr dR-sqr
## (Intercept) 13510.965  1   0.3355     NA
## rdg_fall     5191.112  1   0.1625 0.1485
## sped         1843.901  1   0.0645 0.0527
## 
## Sum of squared errors (SSE): 26763.3
## Sum of squared total  (SST): 34957.4
```

---

```r
anova(m2, sped_mod)
```

```
## Analysis of Variance Table
## 
## Model 1: rdg_spr ~ rdg_fall + sped
## Model 2: rdg_spr ~ sped
##   Res.Df   RSS Df Sum of Sq      F    Pr(&gt;F)    
## 1    171 26763                                  
## 2    172 31954 -1   -5191.1 33.168 3.842e-08 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```

---
# How important is this?
In many ways, this is just a different test of the coefficient (using an `\(F\)`
test versus a `\(t\)` test)

--




```r
anova(m2, sped_mod)$`Pr(&gt;F)`[2]
```

```
## [1] 0.00000003842376
```

```r
summary(m2)$coefficients["rdg_fall", "Pr(&gt;|t|)"]
```

```
## [1] 0.00000003842376
```



--
So we're getting the exact same probability estimate, just using different
criteria and different tests.


---
class: inverse center middle

# A brief foray into information criteria


---
# Comparing models
* Often we may have a set of models we are comparing, and we want to know
  which is "best".
  + Side note "best" can depend on context


--
### Three guiding principles
1. Simplicity and parsimony: "Shave away all but what is necessary"
2. Multiple working hypotheses
  + Rather than a null hypothesis, consider comparing competing theories
3. Strength of Evidence

---
# Akaike Information Criteria

* Rather complicated, and rather useful

--

.Large[

$$
AIC = -2 \times log(L(\hat{\theta}|data)) + 2 \times K
$$

]

--
Luckily, R provides a handy-dandy function to calculate it for you.


```r
AIC(m0)
```

```
## [1] 1420.483
```

```r
AIC(m1)
```

```
## [1] 1387.601
```

```r
AIC(m2)
```

```
## [1] 1378.008
```

---
# What do these values mean?


* Wholly uninterpretable on their own 


--
* Difference between the values are interpretable and can be useful when
  comparing competing models 
    + perhaps representing different theories

--
* Lower AIC values = better fit to the model

--
* The `\(2 \times K\)` part represents a penalization for model complexity

  + As you add variables to model, this value will increase, and so will AIC,
    unless there is a corresponding reduction in, essentially, residual 
    variance


---
# Guidelines `\(\Delta AIC\)` interpretation

Recommendations by Burnham and Anderson (2002)

.gray[That Anderson is not me, obvi]


--
First, rescale the AIC values such that the lowest AIC value is zero

.gray[Note, we're basically centering here]

.Large[
$$
\Delta\_i = AIC\_i - AIC\_{min}
$$
]



1. `\(\Delta_i \leq 2\)`: "Substantial support" (i.e., not much difference in
   the models)
2. `\(4 \leq \Delta_i \leq 7\)`: "Considerably less support" (i.e., gray zone)
3. `\(\Delta_i \gt 10\)`: "Essentially no support" (i.e., reject this model in 
   favor of the model where `\(\Delta_i = 0\)`)


---
# AIC weights

.Large[
$$
w\_i = \frac{exp(-\Delta\_i/2)}{\sum exp(-\Delta\_i/2)}
$$
]

--
What do these end up looking like? Proportions!


--
From Burnham and Anderson
&gt; interpreted as the probability that model i is, in fact, the K-L best model for the data

Where K-L is the Kullback-Leibler information


---
# Evaluating our candidate models

```r
anova(m0, m1, m2)
```

```
## Analysis of Variance Table
## 
## Model 1: rdg_spr ~ 1
## Model 2: rdg_spr ~ rdg_fall
## Model 3: rdg_spr ~ rdg_fall + sped
##   Res.Df   RSS Df Sum of Sq      F    Pr(&gt;F)    
## 1    173 34957                                  
## 2    172 28607  1    6350.2 40.574 1.689e-09 ***
## 3    171 26763  1    1843.9 11.781 0.0007504 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```

---
# Calculate `\(\Delta AIC\)` values

### Compare model 0 and 1

```r
AIC(m1) - AIC(m0)
```

```
## [1] -32.88211
```

--
### Compare model 1 and 2

```r
AIC(m2) - AIC(m1)
```

```
## [1] -9.593069
```

---
# Weights calculation
* A couple different packages, I like mine best


```r
# You may have to install {remotes} first
install.packages("remotes")
remotes::install_github("datalorax/sundry")
```

--

```r
sundry::aic_weights(m0, m1, m2)
```

```
##        predictors     delta weight
## 3 rdg_fall + sped  0.000000   0.99
## 2        rdg_fall  9.593069   0.01
## 1               1 42.475179   0.00
```

---
class: inverse bottom right
background-image:url(../img/bricks.jpg)
background-size:cover

# Blocks of predictors

---
# Load the diagnostics data

```r
diag &lt;- import(here("data", "diagnostics.sav"),
               setclass = "tbl_df") %&gt;% 
  characterize()

head(diag)
```

```
## # A tibble: 6 x 32
##      id area   fsex   fage fheight fweight  ffvc ffev1 msex   mage mheight
##   &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;   &lt;dbl&gt;
## 1     5 Burba… male     46      61     121   354   290 fema…    39      62
## 2     6 Burba… male     44      72     153   610   491 fema…    36      66
## 3    23 Burba… male     51      73     215   499   416 fema…    49      63
## 4    30 Lanca… male     39      68     209   417   362 fema…    36      63
## 5    32 Lanca… male     43      70     196   470   413 fema…    44      67
## 6    34 Lanca… male     33      71     178   586   466 fema…    32      64
## # ... with 21 more variables: mweight &lt;dbl&gt;, fmvc &lt;dbl&gt;, mfev1 &lt;dbl&gt;,
## #   ocsex &lt;chr&gt;, ocage &lt;dbl&gt;, ocheight &lt;dbl&gt;, ocweight &lt;dbl&gt;, ocfvc &lt;dbl&gt;,
## #   ocfev1 &lt;dbl&gt;, mcsex &lt;dbl&gt;, mcage &lt;dbl&gt;, mcheight &lt;dbl&gt;,
## #   mcweight &lt;dbl&gt;, mcfvc &lt;dbl&gt;, mcfev1 &lt;dbl&gt;, ycsex &lt;dbl&gt;, ycage &lt;dbl&gt;,
## #   ycheight &lt;dbl&gt;, ycweight &lt;dbl&gt;, ycfvc &lt;dbl&gt;, ycfev1 &lt;dbl&gt;
```


---
# Enter predictors in blocks
Predict youngest child weight
* Couple of ways we could think about entering the data in blocks.
  + By respondent (i.e., do mother responses contribute beyond father what 
    father responses contribute?)
  + By variable (e.g., "family" weight, height, etc.)
  

--
Let's enter by block for respondent, using weight, height, and age as 
predictors

---
# Competing theories
This is totally made up, but let's pretend there are different camps of people
who think:

--
1. Father's weight matters most for predicting a child's weight 

--
1. Mother's weight matters most for predicting a child's weight 

--
1. Parents contribute jointly in the prediction of a child's weight 

--
1. The weight of all family members contribute jointly in the prediction of a
   child's weight


--
### We can compare these competing theories!

---
# Control block

First, let's control for the age/height of the individual child, so we can see
how much family characteristics contribute beyond the individual's
characteristics.


```r
yc_mod &lt;- lm(ycweight ~ ycage + ycheight, diag)
arm::display(yc_mod, detail = TRUE)
```

```
## lm(formula = ycweight ~ ycage + ycheight, data = diag)
##             coef.est coef.se t value Pr(&gt;|t|)
## (Intercept) -232.01    44.63   -5.20    0.00 
## ycage         -2.94     3.31   -0.89    0.38 
## ycheight       6.13     1.27    4.85    0.00 
## ---
## n = 24, k = 3
*## residual sd = 16.21, R-Squared = 0.78
```

---
# Father Only Block 


```r
father_mod &lt;- lm(ycweight ~ ycage + ycheight + 
                            fage + fheight + fweight,
                 data = diag)
arm::display(father_mod, detail = TRUE)
```

```
## lm(formula = ycweight ~ ycage + ycheight + fage + fheight + fweight, 
##     data = diag)
##             coef.est coef.se t value Pr(&gt;|t|)
## (Intercept) -193.13    95.02   -2.03    0.06 
## ycage         -3.02     4.50   -0.67    0.51 
## ycheight       6.00     1.97    3.04    0.01 
## fage           0.26     0.98    0.26    0.80 
## fheight       -0.87     1.66   -0.52    0.61 
## fweight        0.10     0.19    0.54    0.59 
## ---
## n = 24, k = 6
*## residual sd = 17.31, R-Squared = 0.79
```

---
# Alternative syntax


```r
father_mod &lt;- update(yc_mod, ~ . + fage + fheight + fweight)
```

---
# Mother Only Block

```r
mother_mod &lt;- lm(ycweight ~ ycage + ycheight + 
                            mage + mheight + mweight,
                 data = diag)
arm::display(mother_mod, detail = TRUE)
```

```
## lm(formula = ycweight ~ ycage + ycheight + mage + mheight + mweight, 
##     data = diag)
##             coef.est coef.se t value Pr(&gt;|t|)
## (Intercept) -242.72    91.30   -2.66    0.02 
## ycage         -0.51     3.04   -0.17    0.87 
## ycheight       4.50     1.21    3.72    0.00 
## mage           0.87     0.67    1.31    0.21 
## mheight       -0.05     1.41   -0.03    0.97 
## mweight        0.31     0.10    2.98    0.01 
## ---
## n = 24, k = 6
*## residual sd = 13.99, R-Squared = 0.86
```


---
### Compare

```r
anova(father_mod, yc_mod)
```

```
## Analysis of Variance Table
## 
## Model 1: ycweight ~ ycage + ycheight + fage + fheight + fweight
## Model 2: ycweight ~ ycage + ycheight
##   Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)
## 1     18 5393.9                           
## 2     21 5518.6 -3   -124.61 0.1386 0.9356
```

```r
anova(mother_mod, yc_mod)
```

```
## Analysis of Variance Table
## 
## Model 1: ycweight ~ ycage + ycheight + mage + mheight + mweight
## Model 2: ycweight ~ ycage + ycheight
##   Res.Df    RSS Df Sum of Sq      F  Pr(&gt;F)  
## 1     18 3521.7                              
## 2     21 5518.6 -3   -1996.9 3.4021 0.04027 *
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```


---

```r
sundry::aic_weights(father_mod, mother_mod, yc_mod)
```

```
##                                    predictors     delta weight
## 2 ycage + ycheight + mage + mheight + mweight  0.000000   0.91
## 3                            ycage + ycheight  4.780242   0.08
## 1 ycage + ycheight + fage + fheight + fweight 10.232091   0.01
```

---
# Parent Block

```r
parent_mod &lt;- lm(ycweight ~ ycage + ycheight + 
                            fage  + fheight  + fweight +
                            mage  + mheight  + mweight, 
                 data = diag)
arm::display(parent_mod, detail = TRUE)
```

```
## lm(formula = ycweight ~ ycage + ycheight + fage + fheight + fweight + 
##     mage + mheight + mweight, data = diag)
##             coef.est coef.se t value Pr(&gt;|t|)
## (Intercept) -229.91   103.51   -2.22    0.04 
## ycage         -0.80     3.90   -0.21    0.84 
## ycheight       4.82     1.75    2.76    0.01 
## fage          -1.44     1.34   -1.08    0.30 
## fheight       -0.73     1.50   -0.48    0.64 
## fweight       -0.02     0.17   -0.11    0.91 
## mage           1.80     1.18    1.52    0.15 
## mheight        0.72     1.64    0.44    0.67 
## mweight        0.30     0.11    2.69    0.02 
## ---
## n = 24, k = 9
*## residual sd = 14.61, R-Squared = 0.87
```

---
# Compare

```r
anova(yc_mod, parent_mod)
```

```
## Analysis of Variance Table
## 
## Model 1: ycweight ~ ycage + ycheight
## Model 2: ycweight ~ ycage + ycheight + fage + fheight + fweight + mage + 
##     mheight + mweight
##   Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)
## 1     21 5518.6                           
## 2     15 3203.5  6    2315.1 1.8067 0.1651
```

---

```r
sundry::aic_weights(yc_mod, father_mod, mother_mod, parent_mod)
```

```
##                                                               predictors
## 3                            ycage + ycheight + mage + mheight + mweight
## 4 ycage + ycheight + fage + fheight + fweight + mage + mheight + mweight
## 1                                                       ycage + ycheight
## 2                            ycage + ycheight + fage + fheight + fweight
##       delta weight
## 3  0.000000   0.80
## 4  3.727023   0.12
## 1  4.780242   0.07
## 2 10.232091   0.00
```


---
# Family Block 

```r
family_mod &lt;- lm(ycweight ~ ycage + ycheight +
                            fage  + fheight  + fweight +
                            mage  + mheight  + mweight +
                            ocage + ocheight + ocweight, 
                 data = diag)
```

---

```r
arm::display(family_mod, detail = TRUE)
```

```
## lm(formula = ycweight ~ ycage + ycheight + fage + fheight + fweight + 
##     mage + mheight + mweight + ocage + ocheight + ocweight, data = diag)
##             coef.est coef.se t value Pr(&gt;|t|)
## (Intercept) -209.47   134.54   -1.56    0.15 
## ycage         -1.45     4.35   -0.33    0.74 
## ycheight       4.74     1.92    2.47    0.03 
## fage          -1.80     1.39   -1.29    0.22 
## fheight        1.20     2.07    0.58    0.57 
## fweight       -0.04     0.20   -0.19    0.85 
## mage           1.77     1.36    1.31    0.22 
## mheight        0.24     1.89    0.13    0.90 
## mweight        0.29     0.12    2.52    0.03 
## ocage          4.43     4.01    1.10    0.29 
## ocheight      -2.48     2.23   -1.12    0.29 
## ocweight       0.04     0.34    0.12    0.91 
## ---
## n = 24, k = 12
*## residual sd = 14.81, R-Squared = 0.90
```

---
# Compare

```r
anova(yc_mod, family_mod)
```

```
## Analysis of Variance Table
## 
## Model 1: ycweight ~ ycage + ycheight
## Model 2: ycweight ~ ycage + ycheight + fage + fheight + fweight + mage + 
##     mheight + mweight + ocage + ocheight + ocweight
##   Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)
## 1     21 5518.6                           
## 2     12 2633.3  9    2885.3 1.4609 0.2653
```

---

```r
sundry::aic_weights(yc_mod, father_mod, mother_mod, 
                    parent_mod, family_mod)
```

```
##                                                                                             predictors
## 3                                                          ycage + ycheight + mage + mheight + mweight
## 4                               ycage + ycheight + fage + fheight + fweight + mage + mheight + mweight
## 1                                                                                     ycage + ycheight
## 5 ycage + ycheight + fage + fheight + fweight + mage + mheight + mweight + ocage + ocheight + ocweight
## 2                                                          ycage + ycheight + fage + fheight + fweight
##       delta weight
## 3  0.000000   0.75
## 4  3.727023   0.12
## 1  4.780242   0.07
## 5  5.023025   0.06
## 2 10.232091   0.00
```

---
# What to do from here?
* Remember, we're not talking about this from an exploratory lens, but rather, 
  competing theories. 


--
* The Akaike weights provide us with our primary finding here, which is somewhat
  inconclusive, although most of the evidence points to the model including 
  mother information with the youngest child being the best fit


--
* What if this process led to other ideas for competing models?


 
--
We could fit them, but it would be **Highly** exploratory. Generally, the 
models you fit **should be defined a priori**. Otherwise 🎣


---
class: inverse middle center
# Interactions

---
## Let's fit the ELL interaction model again
Last class we fit the model

$$
Rdg\_{spr\_i} = b\_0 + b\_1(Rdg\_{fall\_i}) + b\_2(ELL\_i) + b\_3(ELL \times Rdg\_{fall\_i})  + e
$$

--
First, center `rdg_fall` so the intercept(s) make sense


```r
benchmarks &lt;- benchmarks %&gt;%
  mutate(rdg_fall_c = rdg_fall - mean(rdg_fall))
```

---
# Fit the model


```r
ell_int &lt;- lm(rdg_spr ~ rdg_fall_c + ell + rdg_fall_c:ell, benchmarks)
arm::display(ell_int, detail = TRUE, digits = 3)
```

```
## lm(formula = rdg_spr ~ rdg_fall_c + ell + rdg_fall_c:ell, data = benchmarks)
##                       coef.est coef.se t value Pr(&gt;|t|)
## (Intercept)           187.719    2.899  64.749   0.000 
## rdg_fall_c              0.039    0.185   0.213   0.831 
## ellMonitor             11.327    3.560   3.182   0.002 
## ellNon-ELL             12.765    3.148   4.055   0.000 
## rdg_fall_c:ellMonitor   0.063    0.287   0.218   0.827 
## rdg_fall_c:ellNon-ELL   0.408    0.203   2.005   0.047 
## ---
## n = 174, k = 6
## residual sd = 12.365, R-Squared = 0.27
```

--
### How would you evaluate if the interaction was "worth it"?

---
# Let's look back at the plot for this model


```r
ggplot(benchmarks, aes(rdg_fall, rdg_spr)) +
  geom_point(color = "gray70") +
  geom_smooth(aes(color = ell),
              method = "lm")
```

![](w8_files/figure-html/ell-plot-1.png)&lt;!-- --&gt;

---
# Compare to a parallel slopes models

--
### Fit parallel slopes model

```r
ell_parallel &lt;- lm(rdg_spr ~ rdg_fall_c + ell, benchmarks)
arm::display(ell_parallel, detail = TRUE)
```

```
## lm(formula = rdg_spr ~ rdg_fall_c + ell, data = benchmarks)
##             coef.est coef.se t value Pr(&gt;|t|)
## (Intercept) 190.87     2.34   81.72    0.00  
## rdg_fall_c    0.34     0.07    4.66    0.00  
## ellMonitor    7.77     3.14    2.47    0.01  
## ellNon-ELL    9.88     2.70    3.66    0.00  
## ---
## n = 174, k = 4
## residual sd = 12.49, R-Squared = 0.24
```

--
Before we actually compare, what's your guess? Which model will display the
better statistical fit?

---

```r
anova(ell_parallel, ell_int)
```

```
## Analysis of Variance Table
## 
## Model 1: rdg_spr ~ rdg_fall_c + ell
## Model 2: rdg_spr ~ rdg_fall_c + ell + rdg_fall_c:ell
##   Res.Df   RSS Df Sum of Sq      F  Pr(&gt;F)  
## 1    170 26513                              
## 2    168 25687  2    825.62 2.6999 0.07013 .
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```

--

```r
sundry::aic_weights(ell_parallel, ell_int)
```

```
##                          predictors   delta weight
## 2 rdg_fall_c + ell + rdg_fall_c:ell 0.00000   0.68
## 1                  rdg_fall_c + ell 1.50457   0.32
```

---
# Another example


```r
glo &lt;- import(here("data", "glo_sim.sav"),
              setclass = "tbl_df") %&gt;%
  characterize()

head(glo)
```

```
## # A tibble: 6 x 14
##   condition         age  aces   sex_risk internalizing externalizing
##   &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;
## 1 Control      15.85763     3 -0.6215645            16            12
## 2 Intervention 15.48255     0  0.4536259             9            13
## 3 Control      13.49213     0 -0.6434958             3             0
## 4 Intervention 14.21766     1 -0.6514939            11            16
## 5 Intervention 17.29500     4  1.151312              9             1
## 6 Intervention 14.80630     3 -0.6855188             9             5
## # ... with 8 more variables: parent_education &lt;chr&gt;, parent_income &lt;chr&gt;,
## #   parent &lt;chr&gt;, teen_eth &lt;chr&gt;, teen_race &lt;chr&gt;, parent_eth &lt;chr&gt;,
## #   parent_race &lt;chr&gt;, id &lt;chr&gt;
```


---
# Block 1: Control variables


```r
glo_m1 &lt;- lm(sex_risk ~ age + 
                        parent_education + parent_income + 
                        parent_eth + parent_race + 
                        teen_eth + teen_race,
             data = glo)
```

--
### If you want to see the coefficients, you'll need to fit the model yourself

---
# Block 2: Main effects


```r
glo_m2 &lt;- lm(sex_risk ~ internalizing + externalizing + aces +
                        age + 
                        parent_education + parent_income + 
                        parent_eth + parent_race + 
                        teen_eth + teen_race,
             data = glo)
```

---
# Block 3: Interactions


```r
glo_m3 &lt;- lm(sex_risk ~ internalizing + externalizing + aces +
                        internalizing:aces + externalizing:aces +
                        age + 
                        parent_education + parent_income + 
                        parent_eth + parent_race + 
                        teen_eth + teen_race,
             data = glo)
```

---
# Compare models

.gray[In reality we'd have probably looked at the coefficients by now too]


```r
anova(glo_m1, glo_m2)
```

```
## Analysis of Variance Table
## 
## Model 1: sex_risk ~ age + parent_education + parent_income + parent_eth + 
##     parent_race + teen_eth + teen_race
## Model 2: sex_risk ~ internalizing + externalizing + aces + age + parent_education + 
##     parent_income + parent_eth + parent_race + teen_eth + teen_race
##   Res.Df    RSS Df Sum of Sq      F     Pr(&gt;F)    
## 1     78 25.588                                   
## 2     75 18.421  3     7.167 9.7267 0.00001695 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```

---

```r
sundry::aic_weights(glo_m1, glo_m2)
```

```
##                                                                                                                        predictors
## 2 internalizing + externalizing + aces + age + parent_education + parent_income + parent_eth + parent_race + teen_eth + teen_race
## 1                                        age + parent_education + parent_income + parent_eth + parent_race + teen_eth + teen_race
##      delta weight
## 2  0.00000      1
## 1 30.14969      0
```

---

.code-bg-red[

```r
as.data.frame(summary(glo_m2)$coefficients) %&gt;%
  head(n = 4)
```

```
##                   Estimate  Std. Error   t value     Pr(&gt;|t|)
## (Intercept)   -3.373527233 0.938970630 -3.592793 0.0005821343
*## internalizing -0.002299817 0.010606983 -0.216821 0.8289365449
*## externalizing  0.030670806 0.007799962  3.932174 0.0001861287
*## aces           0.035991144 0.033224785  1.083262 0.2821625062
```
]


---
# Compare to interaction model


```r
anova(glo_m2, glo_m3)
```

```
## Analysis of Variance Table
## 
## Model 1: sex_risk ~ internalizing + externalizing + aces + age + parent_education + 
##     parent_income + parent_eth + parent_race + teen_eth + teen_race
## Model 2: sex_risk ~ internalizing + externalizing + aces + internalizing:aces + 
##     externalizing:aces + age + parent_education + parent_income + 
##     parent_eth + parent_race + teen_eth + teen_race
##   Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)
## 1     75 18.421                           
## 2     73 17.605  2   0.81634 1.6925 0.1912
```

---

```r
sundry::aic_weights(glo_m1, glo_m2, glo_m3)
```

```
##                                                                                                                                                                  predictors
## 3 internalizing + externalizing + aces + internalizing:aces + externalizing:aces + age + parent_education + parent_income + parent_eth + parent_race + teen_eth + teen_race
## 2                                           internalizing + externalizing + aces + age + parent_education + parent_income + parent_eth + parent_race + teen_eth + teen_race
## 1                                                                                  age + parent_education + parent_income + parent_eth + parent_race + teen_eth + teen_race
##        delta weight
## 3  0.0000000   0.62
## 2  0.9860607   0.38
## 1 31.1357460   0.00
```

---
# A note on over-fitting
* In this case, we have a whole lot of parameters 
  ($k = $37), particularly for only 119
  cases.
* Our model is likely overfit. Entering variables in blocks does not prevent
  overfitting.

---
# Take home messages
* Fitting multiple models can be a powerful method for testing
competing/alternative theories

--
* Comparing competing models can be an efficient way to evaluate interactions
  + Particularly true for categorical variables (i.e., what to do if one of the
  interaction term is significant and another is not)

--
* Information criteria can help aid model comparisons, and actually have
stronger theoretical foundations than `\(p\)` values.

---
# Next week
* We'll talk about predictive modeling
* Themes of information criteria will carry-forward
* We'll finally meet/discuss the infamous "adjusted" `\(R^2\)`

---
class: inverse center middle
# Lab
Focused mostly on comparing models with/without interactions
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="https://platform.twitter.com/widgets.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "atelier-dune-light",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function() {
  var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})();</script>

<script>
(function() {
  var i, text, code, codes = document.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
})();
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
